trainer_name: base
version: 1.0
main_config: &main
    seed: 42
    metrics:

      - type: sklearn_f1
        params:
          average: macro

      - type: sklearn_p
        params:
          average: macro

      - type: sklearn_r
        params:
          average: macro

      - type: sklearn_acc
        params: 
            normalize: True

    device:
        name: 'cuda'

train: &train
    # Change to different value if not using max_steps
    <<: *main
    # Can add the max_epochs vs max_steps functionality
    label_type: logits
    append_text: train
    max_epochs: 5
    loader_params:
        batch_size:  4
        num_workers:  4
        shuffle: True

    optimizer:
        type: adam_w
        params:
            lr: !!float 1e-5
            betas: [0.9,0.998]
            eps: !!float 1e-8

    scheduler: null

    criterion:
        type: CrossEntropyLoss
        params: null

    save_on : ##Best Model/Final Model Saving
      score: sklearn_acc
      desired: min
      best_path: './ckpts/concat/train/best_{}.pth'
      final_path: './ckpts/concat/train/final_{}.pth'

    save_after_epoch: true

    checkpoint:
        checkpoint_dir: './ckpts/concat/'

    log_and_val_interval:  100 ## Set to null if using different val and log intervals, Overrides log_interval nd val_interval

    val_interval:  500 # Global steps
    log:
        log_interval:  100 # Global Steps
        logger_params:
            model: gsamn_answer_att
            trainer: base
            comment: "Train Logs for GSAM Attention with Bert"
            log_dir: './logs/concat/train'
        log_label: 1
        values:
            loss: True
            metrics: True
            hparams: null

val:
    <<: *main
    append_text: val
    max_steps: null
    loader_params:
        batch_size:  4
        num_workers:  4
        shuffle: false

grid_search:
  directory: null
  hyperparams:
    train:
      max_epochs:  10
      loader_params:
        batch_size:
          -  4
      optimizer:
        type:
          - adam_w
          # - adam
        params:
          lr:
            - !!float 1e-4
            - !!float 1e-5
            - !!float 1e-6
          weight_decay:
            - 0
            - 0.001
            - 0.0001
            - 0.00001

      scheduler:
          - null

          - type: reduceplateau
            params:
              mode: min
              factor: !!float 0.1
              patience: !!float 3

          - type: cyclic
            params:
              base_lr: !!float 1e-7
              max_lr: !!float 1e-3
              step_size_up:  100
              step_size_down:  200
              mode: exp_range


          - type: cosineannealrestart
            params:
              T_0:  100
              T_mult:  2
              eta_min: !!float 1e-9



      log:
        logger_params:
          model: answerbert
          trainer: base
          comment: "GridSearch Logs for Answer Attention with Bert"
          log_dir: './logs/gridsearch'
        log_label: null
        values:
          hparams:  ## Can have lists inside hparams
             - name: scheduler_type
               path:
                  - train
                  - scheduler
                  - type
             - name: scheduler_params
               path:
                  - train
                  - scheduler
                  - params
             - name: batch_size
               path:
                  - train
                  - loader_params
                  - batch_size
             - name: optimizer_params
               path:
                 - train
                 - optimizer
                 - params
             - name: optimizer_type
               path:
                 - train
                 - optimizer
                 - type
