{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import wordcloud\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data'\n",
    "TRAIN_DATA_DIR = DATA_DIR+'/train'\n",
    "T1_TRAIN = TRAIN_DATA_DIR+'/Task_1_train.jsonl'\n",
    "T1_DEV = TRAIN_DATA_DIR+'/Task_1_dev.jsonl'\n",
    "T2_TRAIN = TRAIN_DATA_DIR+'/Task_2_train.jsonl'\n",
    "T2_DEV = TRAIN_DATA_DIR+'/Task_2_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [T1_TRAIN,T1_DEV,T2_TRAIN,T2_DEV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "records = {\"Samples\":[],\n",
    "           \"Duplicated Examples\":[],\n",
    "           \"Duplicated Articles\":[],\n",
    "           \"Mean Char Count (Articles)\":[],\n",
    "          \"Std Char Count (Articles)\":[],\n",
    "           \"Max Char Count (Articles)\":[],\n",
    "           \"Min Char Count (Articles)\":[],\n",
    "           \"Mean Word Count (Articles)\":[],\n",
    "           \"Std Word Count (Articles)\":[],\n",
    "           \"Max Word Count (Articles)\":[],\n",
    "           \"Min Word Count (Articles)\":[],\n",
    "           \"Duplicated Questions\":[],\n",
    "           \"Mean Char Count (Questions)\":[],\n",
    "          \"Std Char Count (Questions)\":[],\n",
    "           \"Max Char Count (Questions)\":[],\n",
    "           \"Min Char Count (Questions)\":[],\n",
    "           \"Mean Word Count (Questions)\":[],\n",
    "           \"Std Word Count (Questions)\":[],\n",
    "           \"Max Word Count (Questions)\":[],\n",
    "           \"Min Word Count (Questions)\":[]\n",
    "          }\n",
    "\n",
    "for fil in all_files:\n",
    "\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "\n",
    "    df = pd.read_json(fil,lines=True) ## Can also use jsonlines to read\n",
    "    records[\"Samples\"].append(df.shape[0])\n",
    "    records[\"Duplicated Examples\"].append(df[df.duplicated()].shape[0])\n",
    "\n",
    "    \n",
    "    records[\"Duplicated Articles\"].append(df[df[['article']].duplicated()].shape[0])\n",
    "    article_char_length = df['article'].apply(lambda x : len(x))\n",
    "    records[\"Mean Char Count (Articles)\"].append(article_char_length.mean())\n",
    "    records[\"Std Char Count (Articles)\"].append(article_char_length.std())\n",
    "    records[\"Max Char Count (Articles)\"].append(article_char_length.max())\n",
    "    records[\"Min Char Count (Articles)\"].append(article_char_length.min())\n",
    "    \n",
    "    article_word_length = df['article'].apply(lambda x : len(x.split()))\n",
    "    records[\"Mean Word Count (Articles)\"].append(article_word_length.mean())\n",
    "    records[\"Std Word Count (Articles)\"].append(article_word_length.std())\n",
    "    records[\"Max Word Count (Articles)\"].append(article_word_length.max())\n",
    "    records[\"Min Word Count (Articles)\"].append(article_word_length.min())\n",
    "\n",
    "    records[\"Duplicated Questions\"].append(df[df[['question']].duplicated()].shape[0])\n",
    "    question_char_length = df['question'].apply(lambda x : len(x))\n",
    "    records[\"Mean Char Count (Questions)\"].append(question_char_length.mean())\n",
    "    records[\"Std Char Count (Questions)\"].append(question_char_length.std())\n",
    "    records[\"Max Char Count (Questions)\"].append(question_char_length.max())\n",
    "    records[\"Min Char Count (Questions)\"].append(question_char_length.min())\n",
    "    \n",
    "    question_word_length = df['question'].apply(lambda x : len(x.split()))\n",
    "    records[\"Mean Word Count (Questions)\"].append(question_word_length.mean())\n",
    "    records[\"Std Word Count (Questions)\"].append(question_word_length.std())\n",
    "    records[\"Max Word Count (Questions)\"].append(question_word_length.max())\n",
    "    records[\"Min Word Count (Questions)\"].append(question_word_length.min())\n",
    "    \n",
    "stats_df_words = pd.DataFrame.from_dict(records,columns=columns, orient=\"index\")\n",
    "print(stats_df_words.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Freq Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['article']=df['article'].apply(lambda x : x.lower())\n",
    "    text = df['article'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : word_tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |          41087 |        20447 |          53285 |        25779 |\n",
      "| Total Count        |         482713 |       128258 |         775342 |       203877 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word       |   Count |   Frequency | Word   |   Count |   Frequency | Word   |   Count |   Frequency | Word       |   Count |   Frequency |\n",
      "|----|------------|---------|-------------|--------|---------|-------------|--------|---------|-------------|------------|---------|-------------|\n",
      "|  0 | said       |    7978 |  0.0165274  | said   |    1940 |  0.0151258  | said   |    9594 |  0.0123739  | said       |    2524 |  0.01238    |\n",
      "|  1 | mr         |    2465 |  0.00510655 | mr     |     701 |  0.00546555 | mr     |    3549 |  0.00457733 | mr         |     993 |  0.00487058 |\n",
      "|  2 | would      |    2194 |  0.00454514 | would  |     593 |  0.00462349 | would  |    3539 |  0.00456444 | would      |     954 |  0.00467929 |\n",
      "|  3 | also       |    2019 |  0.00418261 | also   |     506 |  0.00394517 | people |    3335 |  0.00430133 | people     |     872 |  0.00427709 |\n",
      "|  4 | people     |    1897 |  0.00392987 | one    |     486 |  0.00378924 | one    |    3182 |  0.004104   | one        |     814 |  0.0039926  |\n",
      "|  5 | one        |    1705 |  0.00353212 | people |     481 |  0.00375025 | also   |    2784 |  0.00359067 | also       |     675 |  0.00331082 |\n",
      "|  6 | last       |    1461 |  0.00302664 | first  |     395 |  0.00307973 | says   |    2201 |  0.00283875 | two        |     562 |  0.00275656 |\n",
      "|  7 | two        |    1349 |  0.00279462 | last   |     391 |  0.00304854 | two    |    2112 |  0.00272396 | says       |     547 |  0.00268299 |\n",
      "|  8 | first      |    1344 |  0.00278426 | new    |     372 |  0.0029004  | years  |    2084 |  0.00268785 | new        |     545 |  0.00267318 |\n",
      "|  9 | new        |    1341 |  0.00277805 | told   |     349 |  0.00272108 | new    |    2081 |  0.00268398 | time       |     532 |  0.00260942 |\n",
      "| 10 | year       |    1262 |  0.00261439 | two    |     346 |  0.00269769 | time   |    2064 |  0.00266205 | us         |     531 |  0.00260451 |\n",
      "| 11 | years      |    1248 |  0.00258539 | us     |     342 |  0.0026665  | could  |    2002 |  0.00258209 | could      |     517 |  0.00253584 |\n",
      "| 12 | could      |    1222 |  0.00253152 | years  |     340 |  0.00265091 | last   |    1957 |  0.00252405 | years      |     516 |  0.00253094 |\n",
      "| 13 | told       |    1203 |  0.00249216 | year   |     329 |  0.00256514 | first  |    1944 |  0.00250728 | first      |     509 |  0.0024966  |\n",
      "| 14 | us         |    1187 |  0.00245902 | time   |     307 |  0.00239361 | us     |    1820 |  0.00234735 | told       |     493 |  0.00241812 |\n",
      "| 15 | time       |    1159 |  0.00240101 | could  |     291 |  0.00226886 | year   |    1768 |  0.00228028 | year       |     462 |  0.00226607 |\n",
      "| 16 | government |    1036 |  0.0021462  | bbc    |     256 |  0.00199598 | like   |    1727 |  0.0022274  | last       |     455 |  0.00223174 |\n",
      "| 17 | made       |     940 |  0.00194733 | added  |     246 |  0.00191801 | told   |    1659 |  0.0021397  | bbc        |     418 |  0.00205026 |\n",
      "| 18 | bbc        |     894 |  0.00185203 | made   |     236 |  0.00184004 | police |    1504 |  0.00193979 | like       |     408 |  0.00200121 |\n",
      "| 19 | police     |     892 |  0.00184789 | get    |     227 |  0.00176987 | get    |    1476 |  0.00190368 | government |     404 |  0.00198159 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['question']=df['question'].apply(lambda x : x.lower())\n",
    "    text = df['question'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : word_tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |          10032 |         4602 |          11098 |         4966 |\n",
      "| Total Count        |          43683 |        11289 |          47297 |        12255 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word        |   Count |   Frequency | Word        |   Count |   Frequency | Word        |   Count |   Frequency | Word        |   Count |   Frequency |\n",
      "|----|-------------|---------|-------------|-------------|---------|-------------|-------------|---------|-------------|-------------|---------|-------------|\n",
      "|  0 | placeholder |    3227 |  0.0738731  | placeholder |     837 |  0.074143   | placeholder |    3318 |  0.0701524  | placeholder |     851 |  0.069441   |\n",
      "|  1 | said        |     268 |  0.00613511 | new         |      69 |  0.00611214 | new         |     229 |  0.00484174 | new         |      71 |  0.00579355 |\n",
      "|  2 | year        |     230 |  0.00526521 | says        |      62 |  0.00549207 | year        |     218 |  0.00460917 | two         |      60 |  0.00489596 |\n",
      "|  3 | new         |     222 |  0.00508207 | year        |      56 |  0.00496058 | two         |     209 |  0.00441888 | says        |      57 |  0.00465116 |\n",
      "|  4 | says        |     197 |  0.00450976 | us          |      48 |  0.00425193 | one         |     201 |  0.00424974 | year        |      54 |  0.00440636 |\n",
      "|  5 | two         |     172 |  0.00393746 | said        |      46 |  0.00407476 | said        |     200 |  0.0042286  | said        |      54 |  0.00440636 |\n",
      "|  6 | first       |     159 |  0.00363986 | first       |      43 |  0.00380902 | man         |     191 |  0.00403831 | us          |      49 |  0.00399837 |\n",
      "|  7 | world       |     158 |  0.00361697 | two         |      43 |  0.00380902 | police      |     185 |  0.00391145 | people      |      48 |  0.00391677 |\n",
      "|  8 | one         |     148 |  0.00338805 | world       |      38 |  0.00336611 | people      |     177 |  0.00374231 | one         |      47 |  0.00383517 |\n",
      "|  9 | man         |     143 |  0.00327358 | uk          |      35 |  0.00310036 | first       |     174 |  0.00367888 | man         |      46 |  0.00375357 |\n",
      "| 10 | people      |     141 |  0.0032278  | one         |      34 |  0.00301178 | world       |     168 |  0.00355202 | first       |      44 |  0.00359037 |\n",
      "| 11 | us          |     129 |  0.00295309 | man         |      32 |  0.00283462 | years       |     154 |  0.00325602 | years       |      37 |  0.00301918 |\n",
      "| 12 | police      |     127 |  0.00290731 | people      |      32 |  0.00283462 | us          |     147 |  0.00310802 | police      |      36 |  0.00293758 |\n",
      "| 13 | uk          |     127 |  0.00290731 | police      |      31 |  0.00274604 | uk          |     133 |  0.00281202 | uk          |      32 |  0.00261118 |\n",
      "| 14 | wales       |     118 |  0.00270128 | former      |      31 |  0.00274604 | says        |     130 |  0.00274859 | world       |      31 |  0.00252958 |\n",
      "| 15 | years       |     115 |  0.0026326  | england     |      31 |  0.00274604 | three       |     113 |  0.00238916 | could       |      31 |  0.00252958 |\n",
      "| 16 | former      |     112 |  0.00256393 | wales       |      29 |  0.00256887 | england     |     111 |  0.00234687 | city        |      28 |  0.00228478 |\n",
      "| 17 | government  |     110 |  0.00251814 | league      |      29 |  0.00256887 | old         |     105 |  0.00222001 | found       |      27 |  0.00220318 |\n",
      "| 18 | could       |     109 |  0.00249525 | years       |      28 |  0.00248029 | could       |     102 |  0.00215658 | may         |      27 |  0.00220318 |\n",
      "| 19 | city        |     106 |  0.00242657 | three       |      25 |  0.00221455 | last        |     100 |  0.0021143  | bbc         |      27 |  0.00220318 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertTokenizer Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                              |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|------------------------------|----------------|--------------|----------------|--------------|\n",
      "| Mean Token Count (Articles)  |      332.603   |    341.452   |       528.58   |    539.303   |\n",
      "| Std Token Count (Articles)   |      193.729   |    217.673   |       387.756  |    396.286   |\n",
      "| Max Token Count (Articles)   |     2206       |   2043       |      2188      |   2272       |\n",
      "| Min Token Count (Articles)   |       38       |     52       |        46      |     52       |\n",
      "| Mean Token Count (Questions) |       28.6319  |     28.3441  |        30.9629 |     31.1657  |\n",
      "| Std Token Count (Questions)  |        6.83765 |      6.75112 |        10.0225 |      9.98314 |\n",
      "| Max Token Count (Questions)  |       84       |     84       |        91      |     89       |\n",
      "| Min Token Count (Questions)  |        9       |     11       |         9      |      8       |\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "records = {\n",
    "           \"Mean Token Count (Articles)\":[],\n",
    "           \"Std Token Count (Articles)\":[],\n",
    "           \"Max Token Count (Articles)\":[],\n",
    "           \"Min Token Count (Articles)\":[],\n",
    "\n",
    "           \"Mean Token Count (Questions)\":[],\n",
    "           \"Std Token Count (Questions)\":[],\n",
    "           \"Max Token Count (Questions)\":[],\n",
    "           \"Min Token Count (Questions)\":[]\n",
    "          }\n",
    "\n",
    "for fil in all_files:\n",
    "\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "\n",
    "    df = pd.read_json(fil,lines=True) ## Can also use jsonlines to read\n",
    "\n",
    "    article_word_length = df['article'].apply(lambda x : len(tokenizer.tokenize(x)))\n",
    "    records[\"Mean Token Count (Articles)\"].append(article_word_length.mean())\n",
    "    records[\"Std Token Count (Articles)\"].append(article_word_length.std())\n",
    "    records[\"Max Token Count (Articles)\"].append(article_word_length.max())\n",
    "    records[\"Min Token Count (Articles)\"].append(article_word_length.min())\n",
    "\n",
    "    question_word_length = df['question'].apply(lambda x : len(tokenizer.tokenize(x)))\n",
    "    records[\"Mean Token Count (Questions)\"].append(question_word_length.mean())\n",
    "    records[\"Std Token Count (Questions)\"].append(question_word_length.std())\n",
    "    records[\"Max Token Count (Questions)\"].append(question_word_length.max())\n",
    "    records[\"Min Token Count (Questions)\"].append(question_word_length.min())\n",
    "    \n",
    "stats_df_words = pd.DataFrame.from_dict(records,columns=columns, orient=\"index\")\n",
    "print(stats_df_words.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['article']=df['article'].apply(lambda x : x.lower())\n",
    "    text = df['article'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : tokenizer.tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    \n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |          21952 |        15858 |          23842 |        18157 |\n",
      "| Total Count        |         559481 |       148611 |         898160 |       235177 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word   |   Count |   Frequency | Word   |   Count |   Frequency | Word   |   Count |   Frequency | Word   |   Count |   Frequency |\n",
      "|----|--------|---------|-------------|--------|---------|-------------|--------|---------|-------------|--------|---------|-------------|\n",
      "|  0 | ##s    |    8094 |  0.014467   | ##s    |    2393 |  0.0161024  | ##s    |   13511 |  0.015043   | ##s    |    3477 |  0.0147846  |\n",
      "|  1 | said   |    7994 |  0.0142882  | said   |    1941 |  0.0130609  | said   |    9597 |  0.0106852  | said   |    2525 |  0.0107366  |\n",
      "|  2 | mr     |    2466 |  0.00440766 | mr     |     702 |  0.00472374 | ##t    |    4282 |  0.00476752 | ##t    |    1068 |  0.00454126 |\n",
      "|  3 | ##t    |    2266 |  0.00405018 | ##t    |     674 |  0.00453533 | mr     |    3550 |  0.00395253 | mr     |     993 |  0.00422235 |\n",
      "|  4 | would  |    2204 |  0.00393937 | would  |     595 |  0.00400374 | would  |    3549 |  0.00395141 | would  |     964 |  0.00409904 |\n",
      "|  5 | also   |    2020 |  0.00361049 | one    |     513 |  0.00345197 | one    |    3348 |  0.00372762 | people |     873 |  0.0037121  |\n",
      "|  6 | people |    1897 |  0.00339064 | also   |     506 |  0.00340486 | people |    3335 |  0.00371315 | one    |     855 |  0.00363556 |\n",
      "|  7 | one    |    1822 |  0.00325659 | people |     481 |  0.00323664 | also   |    2786 |  0.0031019  | also   |     675 |  0.00287018 |\n",
      "|  8 | two    |    1479 |  0.00264352 | first  |     408 |  0.00274542 | two    |    2277 |  0.00253518 | two    |     611 |  0.00259804 |\n",
      "|  9 | last   |    1477 |  0.00263995 | last   |     399 |  0.00268486 | says   |    2201 |  0.00245057 | new    |     557 |  0.00236843 |\n",
      "| 10 | first  |    1417 |  0.0025327  | new    |     380 |  0.00255701 | new    |    2129 |  0.0023704  | says   |     547 |  0.00232591 |\n",
      "| 11 | new    |    1372 |  0.00245227 | two    |     379 |  0.00255028 | years  |    2084 |  0.0023203  | us     |     546 |  0.00232166 |\n",
      "| 12 | year   |    1281 |  0.00228962 | told   |     349 |  0.00234841 | time   |    2076 |  0.00231139 | time   |     540 |  0.00229614 |\n",
      "| 13 | years  |    1248 |  0.00223064 | us     |     345 |  0.0023215  | first  |    2056 |  0.00228912 | first  |     525 |  0.00223236 |\n",
      "| 14 | could  |    1224 |  0.00218774 | years  |     340 |  0.00228785 | could  |    2007 |  0.00223457 | could  |     518 |  0.0022026  |\n",
      "| 15 | ##m    |    1219 |  0.0021788  | ##e    |     339 |  0.00228112 | last   |    1975 |  0.00219894 | years  |     516 |  0.00219409 |\n",
      "| 16 | us     |    1206 |  0.00215557 | year   |     330 |  0.00222056 | us     |    1844 |  0.00205309 | told   |     493 |  0.00209629 |\n",
      "| 17 | told   |    1203 |  0.00215021 | bbc    |     328 |  0.0022071  | year   |    1793 |  0.0019963  | bbc    |     475 |  0.00201976 |\n",
      "| 18 | time   |    1173 |  0.00209659 | time   |     311 |  0.00209271 | ##ing  |    1785 |  0.0019874  | ##e    |     472 |  0.002007   |\n",
      "| 19 | bbc    |    1112 |  0.00198756 | could  |     291 |  0.00195813 | ##e    |    1761 |  0.00196068 | year   |     468 |  0.00198999 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['question']=df['question'].apply(lambda x : x.lower())\n",
    "    text = df['question'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : tokenizer.tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    \n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |           9771 |         4876 |          10702 |         5251 |\n",
      "| Total Count        |          51629 |        13284 |          55858 |        14331 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word       |   Count |   Frequency | Word     |   Count |   Frequency | Word     |   Count |   Frequency | Word     |   Count |   Frequency |\n",
      "|----|------------|---------|-------------|----------|---------|-------------|----------|---------|-------------|----------|---------|-------------|\n",
      "|  0 | place      |    3256 |  0.0630653  | place    |     844 |  0.0635351  | place    |    3349 |  0.0599556  | place    |     859 |  0.05994    |\n",
      "|  1 | ##holder   |    3227 |  0.0625036  | ##holder |     837 |  0.0630081  | ##holder |    3318 |  0.0594006  | ##holder |     851 |  0.0593818  |\n",
      "|  2 | said       |     272 |  0.00526836 | new      |      69 |  0.00519422 | ##s      |     333 |  0.00596155 | new      |      72 |  0.00502407 |\n",
      "|  3 | year       |     230 |  0.00445486 | ##s      |      66 |  0.00496838 | new      |     230 |  0.00411758 | ##s      |      72 |  0.00502407 |\n",
      "|  4 | ##s        |     223 |  0.00431928 | says     |      62 |  0.00466727 | year     |     218 |  0.00390275 | two      |      60 |  0.00418673 |\n",
      "|  5 | new        |     222 |  0.00429991 | year     |      56 |  0.0042156  | two      |     209 |  0.00374163 | says     |      57 |  0.00397739 |\n",
      "|  6 | says       |     197 |  0.00381568 | us       |      48 |  0.00361337 | one      |     201 |  0.00359841 | year     |      54 |  0.00376806 |\n",
      "|  7 | two        |     172 |  0.00333146 | said     |      46 |  0.00346281 | said     |     200 |  0.00358051 | said     |      54 |  0.00376806 |\n",
      "|  8 | first      |     160 |  0.00309903 | first    |      43 |  0.00323698 | man      |     196 |  0.0035089  | man      |      50 |  0.00348894 |\n",
      "|  9 | world      |     158 |  0.0030603  | two      |      43 |  0.00323698 | police   |     185 |  0.00331197 | us       |      49 |  0.00341916 |\n",
      "| 10 | man        |     150 |  0.00290534 | world    |      38 |  0.00286058 | people   |     177 |  0.00316875 | people   |      48 |  0.00334938 |\n",
      "| 11 | one        |     148 |  0.00286661 | uk       |      36 |  0.00271003 | first    |     174 |  0.00311504 | one      |      47 |  0.0032796  |\n",
      "| 12 | people     |     141 |  0.00273102 | man      |      35 |  0.00263475 | world    |     168 |  0.00300763 | first    |      44 |  0.00307027 |\n",
      "| 13 | uk         |     135 |  0.00261481 | one      |      34 |  0.00255947 | years    |     154 |  0.00275699 | years    |      37 |  0.00258182 |\n",
      "| 14 | us         |     130 |  0.00251796 | people   |      32 |  0.00240891 | us       |     147 |  0.00263167 | police   |      36 |  0.00251204 |\n",
      "| 15 | police     |     129 |  0.0024986  | police   |      31 |  0.00233363 | uk       |     142 |  0.00254216 | uk       |      33 |  0.0023027  |\n",
      "| 16 | wales      |     118 |  0.00228554 | former   |      31 |  0.00233363 | says     |     130 |  0.00232733 | world    |      32 |  0.00223292 |\n",
      "| 17 | years      |     115 |  0.00222743 | england  |      31 |  0.00233363 | three    |     113 |  0.00202299 | could    |      31 |  0.00216314 |\n",
      "| 18 | former     |     112 |  0.00216932 | wales    |      29 |  0.00218308 | england  |     111 |  0.00198718 | city     |      28 |  0.00195381 |\n",
      "| 19 | government |     110 |  0.00213059 | league   |      29 |  0.00218308 | old      |     105 |  0.00187977 | found    |      27 |  0.00188403 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
