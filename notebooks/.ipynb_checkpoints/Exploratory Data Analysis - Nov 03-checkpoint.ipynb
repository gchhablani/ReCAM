{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import wordcloud\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data'\n",
    "TRAIN_DATA_DIR = DATA_DIR+'/train'\n",
    "T1_TRAIN = TRAIN_DATA_DIR+'/Task_1_train.jsonl'\n",
    "T1_DEV = TRAIN_DATA_DIR+'/Task_1_dev.jsonl'\n",
    "T2_TRAIN = TRAIN_DATA_DIR+'/Task_2_train.jsonl'\n",
    "T2_DEV = TRAIN_DATA_DIR+'/Task_2_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [T1_TRAIN,T1_DEV,T2_TRAIN,T2_DEV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                             |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|-----------------------------|----------------|--------------|----------------|--------------|\n",
      "| Samples                     |     3227       |    837       |      3318      |    851       |\n",
      "| Duplicated Examples         |        0       |      0       |         0      |      0       |\n",
      "| Duplicated Articles         |       57       |      4       |        55      |      2       |\n",
      "| Mean Char Count (Articles)  |     1548.47    |   1582.79    |      2448.93   |   2501.41    |\n",
      "| Std Char Count (Articles)   |      905.301   |    992.333   |      1788.42   |   1809.55    |\n",
      "| Max Char Count (Articles)   |    10177       |   9563       |     10370      |   9628       |\n",
      "| Min Char Count (Articles)   |      162       |    255       |       169      |    247       |\n",
      "| Mean Word Count (Articles)  |      262.44    |    268.608   |       417.727  |    427.048   |\n",
      "| Std Word Count (Articles)   |      154.534   |    171.253   |       307.248  |    310.994   |\n",
      "| Max Word Count (Articles)   |     1754       |   1641       |      1700      |   1651       |\n",
      "| Min Word Count (Articles)   |       31       |     41       |        31      |     45       |\n",
      "| Duplicated Questions        |        0       |      0       |         1      |      0       |\n",
      "| Mean Char Count (Questions) |      137.826   |    137.455   |       148.234  |    149.385   |\n",
      "| Std Char Count (Questions)  |       31.1171  |     30.6809  |        47.7494 |     46.9035  |\n",
      "| Max Char Count (Questions)  |      389       |    387       |       443      |    413       |\n",
      "| Min Char Count (Questions)  |       32       |     48       |        31      |     37       |\n",
      "| Mean Word Count (Questions) |       24.6771  |     24.4695  |        26.8852 |     27.2127  |\n",
      "| Std Word Count (Questions)  |        6.18453 |      6.04612 |         9.1822 |      9.16159 |\n",
      "| Max Word Count (Questions)  |       73       |     73       |        83      |     83       |\n",
      "| Min Word Count (Questions)  |        6       |      9       |         7      |      6       |\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "records = {\"Samples\":[],\n",
    "           \"Duplicated Examples\":[],\n",
    "           \"Duplicated Articles\":[],\n",
    "           \"Mean Char Count (Articles)\":[],\n",
    "          \"Std Char Count (Articles)\":[],\n",
    "           \"Max Char Count (Articles)\":[],\n",
    "           \"Min Char Count (Articles)\":[],\n",
    "           \"Mean Word Count (Articles)\":[],\n",
    "           \"Std Word Count (Articles)\":[],\n",
    "           \"Max Word Count (Articles)\":[],\n",
    "           \"Min Word Count (Articles)\":[],\n",
    "           \"Duplicated Questions\":[],\n",
    "           \"Mean Char Count (Questions)\":[],\n",
    "          \"Std Char Count (Questions)\":[],\n",
    "           \"Max Char Count (Questions)\":[],\n",
    "           \"Min Char Count (Questions)\":[],\n",
    "           \"Mean Word Count (Questions)\":[],\n",
    "           \"Std Word Count (Questions)\":[],\n",
    "           \"Max Word Count (Questions)\":[],\n",
    "           \"Min Word Count (Questions)\":[]\n",
    "          }\n",
    "\n",
    "for fil in all_files:\n",
    "\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "\n",
    "    df = pd.read_json(fil,lines=True) ## Can also use jsonlines to read\n",
    "    records[\"Samples\"].append(df.shape[0])\n",
    "    records[\"Duplicated Examples\"].append(df[df.duplicated()].shape[0])\n",
    "\n",
    "    \n",
    "    records[\"Duplicated Articles\"].append(df[df[['article']].duplicated()].shape[0])\n",
    "    article_char_length = df['article'].apply(lambda x : len(x))\n",
    "    records[\"Mean Char Count (Articles)\"].append(article_char_length.mean())\n",
    "    records[\"Std Char Count (Articles)\"].append(article_char_length.std())\n",
    "    records[\"Max Char Count (Articles)\"].append(article_char_length.max())\n",
    "    records[\"Min Char Count (Articles)\"].append(article_char_length.min())\n",
    "    \n",
    "    article_word_length = df['article'].apply(lambda x : len(x.split()))\n",
    "    records[\"Mean Word Count (Articles)\"].append(article_word_length.mean())\n",
    "    records[\"Std Word Count (Articles)\"].append(article_word_length.std())\n",
    "    records[\"Max Word Count (Articles)\"].append(article_word_length.max())\n",
    "    records[\"Min Word Count (Articles)\"].append(article_word_length.min())\n",
    "\n",
    "    records[\"Duplicated Questions\"].append(df[df[['question']].duplicated()].shape[0])\n",
    "    question_char_length = df['question'].apply(lambda x : len(x))\n",
    "    records[\"Mean Char Count (Questions)\"].append(question_char_length.mean())\n",
    "    records[\"Std Char Count (Questions)\"].append(question_char_length.std())\n",
    "    records[\"Max Char Count (Questions)\"].append(question_char_length.max())\n",
    "    records[\"Min Char Count (Questions)\"].append(question_char_length.min())\n",
    "    \n",
    "    question_word_length = df['question'].apply(lambda x : len(x.split()))\n",
    "    records[\"Mean Word Count (Questions)\"].append(question_word_length.mean())\n",
    "    records[\"Std Word Count (Questions)\"].append(question_word_length.std())\n",
    "    records[\"Max Word Count (Questions)\"].append(question_word_length.max())\n",
    "    records[\"Min Word Count (Questions)\"].append(question_word_length.min())\n",
    "    \n",
    "stats_df_words = pd.DataFrame.from_dict(records,columns=columns, orient=\"index\")\n",
    "print(stats_df_words.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Freq Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['article']=df['article'].apply(lambda x : x.lower())\n",
    "    text = df['article'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : word_tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |          41087 |        20447 |          53285 |        25779 |\n",
      "| Total Count        |         482713 |       128258 |         775342 |       203877 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word       |   Count |   Frequency | Word   |   Count |   Frequency | Word   |   Count |   Frequency | Word       |   Count |   Frequency |\n",
      "|----|------------|---------|-------------|--------|---------|-------------|--------|---------|-------------|------------|---------|-------------|\n",
      "|  0 | said       |    7978 |  0.0165274  | said   |    1940 |  0.0151258  | said   |    9594 |  0.0123739  | said       |    2524 |  0.01238    |\n",
      "|  1 | mr         |    2465 |  0.00510655 | mr     |     701 |  0.00546555 | mr     |    3549 |  0.00457733 | mr         |     993 |  0.00487058 |\n",
      "|  2 | would      |    2194 |  0.00454514 | would  |     593 |  0.00462349 | would  |    3539 |  0.00456444 | would      |     954 |  0.00467929 |\n",
      "|  3 | also       |    2019 |  0.00418261 | also   |     506 |  0.00394517 | people |    3335 |  0.00430133 | people     |     872 |  0.00427709 |\n",
      "|  4 | people     |    1897 |  0.00392987 | one    |     486 |  0.00378924 | one    |    3182 |  0.004104   | one        |     814 |  0.0039926  |\n",
      "|  5 | one        |    1705 |  0.00353212 | people |     481 |  0.00375025 | also   |    2784 |  0.00359067 | also       |     675 |  0.00331082 |\n",
      "|  6 | last       |    1461 |  0.00302664 | first  |     395 |  0.00307973 | says   |    2201 |  0.00283875 | two        |     562 |  0.00275656 |\n",
      "|  7 | two        |    1349 |  0.00279462 | last   |     391 |  0.00304854 | two    |    2112 |  0.00272396 | says       |     547 |  0.00268299 |\n",
      "|  8 | first      |    1344 |  0.00278426 | new    |     372 |  0.0029004  | years  |    2084 |  0.00268785 | new        |     545 |  0.00267318 |\n",
      "|  9 | new        |    1341 |  0.00277805 | told   |     349 |  0.00272108 | new    |    2081 |  0.00268398 | time       |     532 |  0.00260942 |\n",
      "| 10 | year       |    1262 |  0.00261439 | two    |     346 |  0.00269769 | time   |    2064 |  0.00266205 | us         |     531 |  0.00260451 |\n",
      "| 11 | years      |    1248 |  0.00258539 | us     |     342 |  0.0026665  | could  |    2002 |  0.00258209 | could      |     517 |  0.00253584 |\n",
      "| 12 | could      |    1222 |  0.00253152 | years  |     340 |  0.00265091 | last   |    1957 |  0.00252405 | years      |     516 |  0.00253094 |\n",
      "| 13 | told       |    1203 |  0.00249216 | year   |     329 |  0.00256514 | first  |    1944 |  0.00250728 | first      |     509 |  0.0024966  |\n",
      "| 14 | us         |    1187 |  0.00245902 | time   |     307 |  0.00239361 | us     |    1820 |  0.00234735 | told       |     493 |  0.00241812 |\n",
      "| 15 | time       |    1159 |  0.00240101 | could  |     291 |  0.00226886 | year   |    1768 |  0.00228028 | year       |     462 |  0.00226607 |\n",
      "| 16 | government |    1036 |  0.0021462  | bbc    |     256 |  0.00199598 | like   |    1727 |  0.0022274  | last       |     455 |  0.00223174 |\n",
      "| 17 | made       |     940 |  0.00194733 | added  |     246 |  0.00191801 | told   |    1659 |  0.0021397  | bbc        |     418 |  0.00205026 |\n",
      "| 18 | bbc        |     894 |  0.00185203 | made   |     236 |  0.00184004 | police |    1504 |  0.00193979 | like       |     408 |  0.00200121 |\n",
      "| 19 | police     |     892 |  0.00184789 | get    |     227 |  0.00176987 | get    |    1476 |  0.00190368 | government |     404 |  0.00198159 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['question']=df['question'].apply(lambda x : x.lower())\n",
    "    text = df['question'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : word_tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |          10032 |         4602 |          11098 |         4966 |\n",
      "| Total Count        |          43683 |        11289 |          47297 |        12255 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word        |   Count |   Frequency | Word        |   Count |   Frequency | Word        |   Count |   Frequency | Word        |   Count |   Frequency |\n",
      "|----|-------------|---------|-------------|-------------|---------|-------------|-------------|---------|-------------|-------------|---------|-------------|\n",
      "|  0 | placeholder |    3227 |  0.0738731  | placeholder |     837 |  0.074143   | placeholder |    3318 |  0.0701524  | placeholder |     851 |  0.069441   |\n",
      "|  1 | said        |     268 |  0.00613511 | new         |      69 |  0.00611214 | new         |     229 |  0.00484174 | new         |      71 |  0.00579355 |\n",
      "|  2 | year        |     230 |  0.00526521 | says        |      62 |  0.00549207 | year        |     218 |  0.00460917 | two         |      60 |  0.00489596 |\n",
      "|  3 | new         |     222 |  0.00508207 | year        |      56 |  0.00496058 | two         |     209 |  0.00441888 | says        |      57 |  0.00465116 |\n",
      "|  4 | says        |     197 |  0.00450976 | us          |      48 |  0.00425193 | one         |     201 |  0.00424974 | year        |      54 |  0.00440636 |\n",
      "|  5 | two         |     172 |  0.00393746 | said        |      46 |  0.00407476 | said        |     200 |  0.0042286  | said        |      54 |  0.00440636 |\n",
      "|  6 | first       |     159 |  0.00363986 | first       |      43 |  0.00380902 | man         |     191 |  0.00403831 | us          |      49 |  0.00399837 |\n",
      "|  7 | world       |     158 |  0.00361697 | two         |      43 |  0.00380902 | police      |     185 |  0.00391145 | people      |      48 |  0.00391677 |\n",
      "|  8 | one         |     148 |  0.00338805 | world       |      38 |  0.00336611 | people      |     177 |  0.00374231 | one         |      47 |  0.00383517 |\n",
      "|  9 | man         |     143 |  0.00327358 | uk          |      35 |  0.00310036 | first       |     174 |  0.00367888 | man         |      46 |  0.00375357 |\n",
      "| 10 | people      |     141 |  0.0032278  | one         |      34 |  0.00301178 | world       |     168 |  0.00355202 | first       |      44 |  0.00359037 |\n",
      "| 11 | us          |     129 |  0.00295309 | man         |      32 |  0.00283462 | years       |     154 |  0.00325602 | years       |      37 |  0.00301918 |\n",
      "| 12 | police      |     127 |  0.00290731 | people      |      32 |  0.00283462 | us          |     147 |  0.00310802 | police      |      36 |  0.00293758 |\n",
      "| 13 | uk          |     127 |  0.00290731 | police      |      31 |  0.00274604 | uk          |     133 |  0.00281202 | uk          |      32 |  0.00261118 |\n",
      "| 14 | wales       |     118 |  0.00270128 | former      |      31 |  0.00274604 | says        |     130 |  0.00274859 | world       |      31 |  0.00252958 |\n",
      "| 15 | years       |     115 |  0.0026326  | england     |      31 |  0.00274604 | three       |     113 |  0.00238916 | could       |      31 |  0.00252958 |\n",
      "| 16 | former      |     112 |  0.00256393 | wales       |      29 |  0.00256887 | england     |     111 |  0.00234687 | city        |      28 |  0.00228478 |\n",
      "| 17 | government  |     110 |  0.00251814 | league      |      29 |  0.00256887 | old         |     105 |  0.00222001 | found       |      27 |  0.00220318 |\n",
      "| 18 | could       |     109 |  0.00249525 | years       |      28 |  0.00248029 | could       |     102 |  0.00215658 | may         |      27 |  0.00220318 |\n",
      "| 19 | city        |     106 |  0.00242657 | three       |      25 |  0.00221455 | last        |     100 |  0.0021143  | bbc         |      27 |  0.00220318 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertTokenizer Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>question</th>\n",
       "      <th>option_0</th>\n",
       "      <th>option_1</th>\n",
       "      <th>option_2</th>\n",
       "      <th>option_3</th>\n",
       "      <th>option_4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Trainspotting author has agreed to become ...</td>\n",
       "      <td>Irvine Welsh is to spearhead a campaign aimed ...</td>\n",
       "      <td>neglected</td>\n",
       "      <td>renewed</td>\n",
       "      <td>lavish</td>\n",
       "      <td>revised</td>\n",
       "      <td>proposed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The economy of the 28-nation EU is set to grow...</td>\n",
       "      <td>The economic recovery within the European Unio...</td>\n",
       "      <td>special</td>\n",
       "      <td>fair</td>\n",
       "      <td>remarkable</td>\n",
       "      <td>modest</td>\n",
       "      <td>similar</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The facility, based on the town's Crichton est...</td>\n",
       "      <td>Councillors are to be given a progress report ...</td>\n",
       "      <td>temporary</td>\n",
       "      <td>controversial</td>\n",
       "      <td>lasting</td>\n",
       "      <td>national</td>\n",
       "      <td>special</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Scottish golf club said a recent consultat...</td>\n",
       "      <td>Royal Troon members have voted \" overwhelmingl...</td>\n",
       "      <td>special</td>\n",
       "      <td>past</td>\n",
       "      <td>forthcoming</td>\n",
       "      <td>potential</td>\n",
       "      <td>prestigious</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Former Aberdeen and Manchester United manager ...</td>\n",
       "      <td>Sir Alex Ferguson and Annie Lennox are among t...</td>\n",
       "      <td>latest</td>\n",
       "      <td>free</td>\n",
       "      <td>best</td>\n",
       "      <td>major</td>\n",
       "      <td>famous</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>A total of £300,000 is needed to save the piec...</td>\n",
       "      <td>An appeal has been launched to save a 17th Cen...</td>\n",
       "      <td>closure</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>influence</td>\n",
       "      <td>realism</td>\n",
       "      <td>value</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>Pupils came up with the idea of the parking no...</td>\n",
       "      <td>Parents who park or drive @placeholder near a ...</td>\n",
       "      <td>badly</td>\n",
       "      <td>mistakenly</td>\n",
       "      <td>by</td>\n",
       "      <td>in</td>\n",
       "      <td>anywhere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>Lendl Simmons (102) shared 154 with Darren Sam...</td>\n",
       "      <td>Ireland caused the first shock of the World Cu...</td>\n",
       "      <td>loss</td>\n",
       "      <td>haul</td>\n",
       "      <td>deficit</td>\n",
       "      <td>qualifiers</td>\n",
       "      <td>triumph</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>Messi and his father Jorge are both accused of...</td>\n",
       "      <td>Argentina and Barcelona star Lionel Messi has ...</td>\n",
       "      <td>latest</td>\n",
       "      <td>professional</td>\n",
       "      <td>fresh</td>\n",
       "      <td>own</td>\n",
       "      <td>immediate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>Mr Justice Mostyn said the game could be said ...</td>\n",
       "      <td>The card game Bridge is \" @placeholder \" a spo...</td>\n",
       "      <td>arguably</td>\n",
       "      <td>simply</td>\n",
       "      <td>virtually</td>\n",
       "      <td>urgently</td>\n",
       "      <td>truly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3227 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  \\\n",
       "0     The Trainspotting author has agreed to become ...   \n",
       "1     The economy of the 28-nation EU is set to grow...   \n",
       "2     The facility, based on the town's Crichton est...   \n",
       "3     The Scottish golf club said a recent consultat...   \n",
       "4     Former Aberdeen and Manchester United manager ...   \n",
       "...                                                 ...   \n",
       "3222  A total of £300,000 is needed to save the piec...   \n",
       "3223  Pupils came up with the idea of the parking no...   \n",
       "3224  Lendl Simmons (102) shared 154 with Darren Sam...   \n",
       "3225  Messi and his father Jorge are both accused of...   \n",
       "3226  Mr Justice Mostyn said the game could be said ...   \n",
       "\n",
       "                                               question   option_0  \\\n",
       "0     Irvine Welsh is to spearhead a campaign aimed ...  neglected   \n",
       "1     The economic recovery within the European Unio...    special   \n",
       "2     Councillors are to be given a progress report ...  temporary   \n",
       "3     Royal Troon members have voted \" overwhelmingl...    special   \n",
       "4     Sir Alex Ferguson and Annie Lennox are among t...     latest   \n",
       "...                                                 ...        ...   \n",
       "3222  An appeal has been launched to save a 17th Cen...    closure   \n",
       "3223  Parents who park or drive @placeholder near a ...      badly   \n",
       "3224  Ireland caused the first shock of the World Cu...       loss   \n",
       "3225  Argentina and Barcelona star Lionel Messi has ...     latest   \n",
       "3226  The card game Bridge is \" @placeholder \" a spo...   arguably   \n",
       "\n",
       "           option_1     option_2    option_3     option_4  label  \n",
       "0           renewed       lavish     revised     proposed      0  \n",
       "1              fair   remarkable      modest      similar      3  \n",
       "2     controversial      lasting    national      special      3  \n",
       "3              past  forthcoming   potential  prestigious      0  \n",
       "4              free         best       major       famous      4  \n",
       "...             ...          ...         ...          ...    ...  \n",
       "3222       accuracy    influence     realism        value      3  \n",
       "3223     mistakenly           by          in     anywhere      0  \n",
       "3224           haul      deficit  qualifiers      triumph      4  \n",
       "3225   professional        fresh         own    immediate      0  \n",
       "3226         simply    virtually    urgently        truly      0  \n",
       "\n",
       "[3227 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(all_files[0],lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f609d6b9ba545b5b2d5bc965b00affc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e41c93adbf4fc6a046b7ed6b5b5d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer\n",
    "tok2 = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16135,)\n",
      "(4185,)\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "records = {\n",
    "           \"Mean Token Count (Articles)\":[],\n",
    "           \"Std Token Count (Articles)\":[],\n",
    "           \"Max Token Count (Articles)\":[],\n",
    "           \"Min Token Count (Articles)\":[],\n",
    "\n",
    "           \"Mean Token Count (Questions)\":[],\n",
    "           \"Std Token Count (Questions)\":[],\n",
    "           \"Max Token Count (Questions)\":[],\n",
    "           \"Min Token Count (Questions)\":[],\n",
    "    \n",
    "           \"Mean Token Count (Answers) LF\":[],\n",
    "           \"Std Token Count (Answers) LF\":[],\n",
    "           \"Max Token Count (Answers) LF\":[],\n",
    "           \"Min Token Count (Answers) LF\":[],\n",
    "\n",
    "          }\n",
    "\n",
    "for fil in all_files:\n",
    "\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "\n",
    "    df = pd.read_json(fil,lines=True) ## Can also use jsonlines to read\n",
    "\n",
    "    article_word_length = df['article'].apply(lambda x : len(tokenizer.tokenize(x)))\n",
    "    records[\"Mean Token Count (Articles)\"].append(article_word_length.mean())\n",
    "    records[\"Std Token Count (Articles)\"].append(article_word_length.std())\n",
    "    records[\"Max Token Count (Articles)\"].append(article_word_length.max())\n",
    "    records[\"Min Token Count (Articles)\"].append(article_word_length.min())\n",
    "\n",
    "    question_word_length = df['question'].apply(lambda x : len(tokenizer.tokenize(x)))\n",
    "    records[\"Mean Token Count (Questions)\"].append(question_word_length.mean())\n",
    "    records[\"Std Token Count (Questions)\"].append(question_word_length.std())\n",
    "    records[\"Max Token Count (Questions)\"].append(question_word_length.max())\n",
    "    records[\"Min Token Count (Questions)\"].append(question_word_length.min())\n",
    "    \n",
    "    answer_word_length = df['option_0'].apply(lambda x : len(tok2.tokenize(x)))\n",
    "    \n",
    "    answer_word_length = pd.concat([answer_word_length,df['option_1'].apply(lambda x : len(tok2.tokenize(x)))])\n",
    "    answer_word_length = pd.concat([answer_word_length,df['option_2'].apply(lambda x : len(tok2.tokenize(x)))])\n",
    "    answer_word_length = pd.concat([answer_word_length,df['option_3'].apply(lambda x : len(tok2.tokenize(x)))])\n",
    "    answer_word_length = pd.concat([answer_word_length,df['option_4'].apply(lambda x : len(tok2.tokenize(x)))])\n",
    "#     print(answer_word_length.shape)\n",
    "    records[\"Mean Token Count (Answers) LF\"].append(answer_word_length.mean())\n",
    "    records[\"Std Token Count (Answers) LF\"].append(answer_word_length.std())\n",
    "    records[\"Max Token Count (Answers) LF\"].append(answer_word_length.max())\n",
    "    records[\"Min Token Count (Answers) LF\"].append(answer_word_length.min())\n",
    "    \n",
    "stats_df_words = pd.DataFrame.from_dict(records,columns=columns, orient=\"index\")\n",
    "print(stats_df_words.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['article']=df['article'].apply(lambda x : x.lower())\n",
    "    text = df['article'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : tokenizer.tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    \n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Unique Token Count\":[],\"Total Count\":[]}\n",
    "columns = []\n",
    "all_freq_df = None\n",
    "for fil in all_files:\n",
    "    columns.append(fil.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    df = pd.read_json(fil,lines=True)\n",
    "    df['question']=df['question'].apply(lambda x : x.lower())\n",
    "    text = df['question'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    text = text.apply(lambda x : tokenizer.tokenize(x))\n",
    "    text = text.apply(lambda x: np.array([y.lower() for y in x if y.lower() not in sw])).values\n",
    "    \n",
    "    tokencount = {}\n",
    "    for split_sentence in text:\n",
    "        for token in split_sentence:\n",
    "            if(token in tokencount):\n",
    "                tokencount[token]+=1\n",
    "            else:\n",
    "                tokencount[token]=1\n",
    "    \n",
    "    counts[\"Unique Token Count\"].append(len(tokencount))\n",
    "    token_counter = Counter(tokencount)\n",
    "    total_count = np.sum(list(tokencount.values()))\n",
    "    counts[\"Total Count\"].append(total_count)\n",
    "    #print(\"Most Common Words\")\n",
    "    # print(\"=\"*30)\n",
    "   \n",
    "\n",
    "    rows = {\"Word\":[],\"Count\":[],\"Frequency\":[]}\n",
    "    for token,count in token_counter.most_common(20):\n",
    "        #print(f\"{token}: {count}, {count/total_count}\")\n",
    "        rows[\"Word\"].append(token)\n",
    "        rows[\"Count\"].append(count)\n",
    "        rows[\"Frequency\"].append(count/total_count)\n",
    "\n",
    "    freq_words_df = pd.DataFrame.from_dict(rows)\n",
    "    if(all_freq_df is None):\n",
    "        all_freq_df = freq_words_df\n",
    "    else:\n",
    "        all_freq_df = pd.concat([all_freq_df,freq_words_df],axis=1)\n",
    "    \n",
    "counts_df = pd.DataFrame.from_dict(counts,orient=\"index\",columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                    |   Task_1_train |   Task_1_dev |   Task_2_train |   Task_2_dev |\n",
      "|--------------------|----------------|--------------|----------------|--------------|\n",
      "| Unique Token Count |           9771 |         4876 |          10702 |         5251 |\n",
      "| Total Count        |          51629 |        13284 |          55858 |        14331 |\n"
     ]
    }
   ],
   "source": [
    "print(counts_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Word       |   Count |   Frequency | Word     |   Count |   Frequency | Word     |   Count |   Frequency | Word     |   Count |   Frequency |\n",
      "|----|------------|---------|-------------|----------|---------|-------------|----------|---------|-------------|----------|---------|-------------|\n",
      "|  0 | place      |    3256 |  0.0630653  | place    |     844 |  0.0635351  | place    |    3349 |  0.0599556  | place    |     859 |  0.05994    |\n",
      "|  1 | ##holder   |    3227 |  0.0625036  | ##holder |     837 |  0.0630081  | ##holder |    3318 |  0.0594006  | ##holder |     851 |  0.0593818  |\n",
      "|  2 | said       |     272 |  0.00526836 | new      |      69 |  0.00519422 | ##s      |     333 |  0.00596155 | new      |      72 |  0.00502407 |\n",
      "|  3 | year       |     230 |  0.00445486 | ##s      |      66 |  0.00496838 | new      |     230 |  0.00411758 | ##s      |      72 |  0.00502407 |\n",
      "|  4 | ##s        |     223 |  0.00431928 | says     |      62 |  0.00466727 | year     |     218 |  0.00390275 | two      |      60 |  0.00418673 |\n",
      "|  5 | new        |     222 |  0.00429991 | year     |      56 |  0.0042156  | two      |     209 |  0.00374163 | says     |      57 |  0.00397739 |\n",
      "|  6 | says       |     197 |  0.00381568 | us       |      48 |  0.00361337 | one      |     201 |  0.00359841 | year     |      54 |  0.00376806 |\n",
      "|  7 | two        |     172 |  0.00333146 | said     |      46 |  0.00346281 | said     |     200 |  0.00358051 | said     |      54 |  0.00376806 |\n",
      "|  8 | first      |     160 |  0.00309903 | first    |      43 |  0.00323698 | man      |     196 |  0.0035089  | man      |      50 |  0.00348894 |\n",
      "|  9 | world      |     158 |  0.0030603  | two      |      43 |  0.00323698 | police   |     185 |  0.00331197 | us       |      49 |  0.00341916 |\n",
      "| 10 | man        |     150 |  0.00290534 | world    |      38 |  0.00286058 | people   |     177 |  0.00316875 | people   |      48 |  0.00334938 |\n",
      "| 11 | one        |     148 |  0.00286661 | uk       |      36 |  0.00271003 | first    |     174 |  0.00311504 | one      |      47 |  0.0032796  |\n",
      "| 12 | people     |     141 |  0.00273102 | man      |      35 |  0.00263475 | world    |     168 |  0.00300763 | first    |      44 |  0.00307027 |\n",
      "| 13 | uk         |     135 |  0.00261481 | one      |      34 |  0.00255947 | years    |     154 |  0.00275699 | years    |      37 |  0.00258182 |\n",
      "| 14 | us         |     130 |  0.00251796 | people   |      32 |  0.00240891 | us       |     147 |  0.00263167 | police   |      36 |  0.00251204 |\n",
      "| 15 | police     |     129 |  0.0024986  | police   |      31 |  0.00233363 | uk       |     142 |  0.00254216 | uk       |      33 |  0.0023027  |\n",
      "| 16 | wales      |     118 |  0.00228554 | former   |      31 |  0.00233363 | says     |     130 |  0.00232733 | world    |      32 |  0.00223292 |\n",
      "| 17 | years      |     115 |  0.00222743 | england  |      31 |  0.00233363 | three    |     113 |  0.00202299 | could    |      31 |  0.00216314 |\n",
      "| 18 | former     |     112 |  0.00216932 | wales    |      29 |  0.00218308 | england  |     111 |  0.00198718 | city     |      28 |  0.00195381 |\n",
      "| 19 | government |     110 |  0.00213059 | league   |      29 |  0.00218308 | old      |     105 |  0.00187977 | found    |      27 |  0.00188403 |\n"
     ]
    }
   ],
   "source": [
    "print(all_freq_df.to_markdown(tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
